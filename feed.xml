<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="/cognitive-labs/feed.xml" rel="self" type="application/atom+xml" /><link href="/cognitive-labs/" rel="alternate" type="text/html" /><updated>2025-07-04T18:32:54+00:00</updated><id>/cognitive-labs/feed.xml</id><title type="html">Cognitive Labs - Ericsson</title><subtitle>Ericsson Cognitive Labs, opening our AI Research to the world</subtitle><entry><title type="html">The Case for Framework-Agnostic Development in ML/DL Software Systems</title><link href="/cognitive-labs/2025/04/24/multi-backend-post.html" rel="alternate" type="text/html" title="The Case for Framework-Agnostic Development in ML/DL Software Systems" /><published>2025-04-24T00:00:00+00:00</published><updated>2025-07-04T18:32:07+00:00</updated><id>/cognitive-labs/2025/04/24/multi-backend-post</id><content type="html" xml:base="/cognitive-labs/2025/04/24/multi-backend-post.html"><![CDATA[<p>The exponential growth of deep learning has positioned AI-driven software systems at the forefront of corporate strategy<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. With this economic force and promising results, deep learning has exploded in capability and complexity, leading to an unstandardized development of tools and frameworks.
Machine learning engineers today face a proliferation of frameworks – from TensorFlow and PyTorch to JAX, MXNet, and beyond. Each framework comes with its own APIs, execution paradigms, and ecosystem of tools. This variety has sparked “framework wars”, where teams struggle to choose the right tool for each project without even knowing how it will develop.</p>

<script type="text/javascript" src="https://ssl.gstatic.com/trends_nrtr/4031_RC01/embed_loader.js"></script>
<script type="text/javascript"> trends.embed.renderExploreWidget("TIMESERIES", {"comparisonItem":[{"keyword":"/g/11gd3905v1","geo":"","time":"2014-01-01 2025-04-29"},{"keyword":"/g/11bwp1s2k3","geo":"","time":"2014-01-01 2025-04-29"},{"keyword":"/m/0h95mh8","geo":"","time":"2014-01-01 2025-04-29"},{"keyword":"/g/11t6my1_gw","geo":"","time":"2014-01-01 2025-04-29"}],"category":0,"property":""}, {"exploreQuery":"date=2014-01-01%202025-04-29&q=%2Fg%2F11gd3905v1,%2Fg%2F11bwp1s2k3,%2Fm%2F0h95mh8,%2Fg%2F11t6my1_gw&hl=en-GB","guestPath":"https://trends.google.com:443/trends/embed/"}); </script>

<p>The landscape shifts quickly as the frameworks coexist, emerge and disappear over time, each with distinct faults, development patterns, and learning curves. And the image is further complicated by the physical backend dependencies of machine learning pipelines, as seen in the well-known landscape of deep learning hidden debt paper from 2015 in NeurIPS, Figure 1. An bringing it to more recent days with growing tooling maturity to cover some sections, recent studies still highlight technical debt even at the framework level — including design, compatibility, and algorithmic debt across major deep learning libraries<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>.</p>

<p><img src="/images/posts/hidden_debt.svg#center" alt="Hidden Technical Debt, NIPS’15" /></p>

<p>These realities contribute to a broader organizational gap: although over 90% of companies plan to increase AI investments in the next three years, only 1% currently feel they have fully mature AI capabilities; one major factor -  fragmentation and instability at the foundational level of machine learning infrastructure.</p>

<p>In today’s actual picture, different stakeholders gravitate towards different frameworks: researchers frequently prototype in PyTorch for flexibility, the industry favours JAX for high-performance scaling due to large-scale model/data parallelism, and deployment is often executed on Tensorflow or ONNX  prioritizing cross-platform compatibility across CPUs, GPUs, and mobile devices. Integrating open-source models into enterprise solutions<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> further complicates the ecosystem, as developers frequently blend external and proprietary components into their AI portfolios.</p>

<p>Adding up all these preferences, open-sourced serialized models, and common practices lead to bottlenecks and impact three critical dimensions: reproducibility, portability, and future maintainability for all stages from research to product.</p>

<p>Framework-agnostic and multi-framework support approaches are emerging as important strategies, paired with better toolchains, to address the challenges of reproducibility, deployment flexibility, and evolving infrastructure demands. Interoperability efforts also enable teams to leverage the best tool for each specific task more easily, reducing the impact of framework-specific limitations over time.</p>

<ul>
  <li>
    <p><strong>avoiding lock-in</strong>  - Avoids complete dependency on the community of that library, best example: Theano or TensorFlow v1.X. Using high-level libraries that support multiple backends ensures the stability of the project through years and new structural changes on big libraries, by loss of community, change direction or external introduction of breaking changes.</p>
  </li>
  <li>
    <p><strong>leveraging best-of-breed tools</strong> - Each framework historically had its strengths, which has been the main reason for the fragmented ecosystem, and while PyTorch is closing the gap, there are still performance improvements on other libraries more specific to use cases, such as physics simulations or serving infrastructures, allowing interoperability without or with minimal code changing.</p>
  </li>
  <li>
    <p><strong>reproducibility across labs and infrastructure</strong> - Moving models between frameworks today — even from TensorFlow to JAX or PyTorch to ONNX — requires careful handling of graph execution modes, random number generators (PRNGs), custom gradient logic, tensor shapes, and XLA compilation behaviours. Framework-agnostic systems reduce these friction points, facilitating faster collaboration across research groups, product teams, and deployment environments.</p>
  </li>
  <li>
    <p><strong>deployment flexibility</strong> - In modern enterprises, models often traverse diverse infrastructures, training on cloud GPUs, optimizing for mobile inference, serving at the edge, or scaling across distributed clusters. Framework-agnostic development, supported by common serialization formats like ONNX or SavedModel, allows seamless transitions and broadens deployment options.</p>
  </li>
</ul>

<p>Without an intentional move toward agnosticism, organizations risk accumulating technical debt that could bottleneck their innovation, scaling, and maintenance efforts for years.</p>

<p>In the present, the boundary between frameworks is not as rigid as before. PyTorch and TensorFlow have influenced each other: TensorFlow 2 adopted PyTorch-like eager execution, and PyTorch 2.0 introduced an optionally compiled mode (torch.compile) to optimize models much like XLA does for TensorFlow/JAX. Google’s OpenXLA project (open-sourcing the XLA compiler and encouraging community contribution) creates a shared compiler backend for JAX, TensorFlow, and PyTorch (via Torch XLA). If successful, these frameworks could eventually rely on the same lower-level compiler for graphs, leading to more consistent performance and easier cross-framework equivalence. Similarly, there is collaboration on defining standard op sets (e.g., TensorFlow and PyTorch teams working with ONNX to ensure new ops can round-trip). All of this industry effort is gradually moving us toward a world where the choice of framework is more about user preference and less about fundamental capability gaps.</p>

<p>On the other hand, high-level libraries continue to emerge — often tied tightly to specific backends<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>. These high-level libraries, being open source, are inherently subject to issues — both within their own codebases and through their dependencies on backend-specific functionality. The main direction of these libraries should transition towards multi-backend support. The best well-known early adopter of high-level APIs is <a href="https://keras.io/keras_3/">Keras 3.0</a>, which reimagined itself as a truly backend-agnostic API, able to run the exact model code on TensorFlow, JAX, and or PyTorch. ONNX Runtime offers flexible execution of exported models across CPU, GPU, and specialized accelerators; Hugging Face Diffusers now supports model training and inference across PyTorch and TensorFlow; or Flower enables federated learning across heterogeneous clients, abstracting backend differences.</p>

<!-- Landscape embed view -->
<iframe src="https://landscape.lfaidata.foundation/embed/embed.html?base-path=&amp;classify=category&amp;key=deep-learning&amp;headers=true&amp;category-header=true&amp;category-in-subcategory=false&amp;title-uppercase=false&amp;title-alignment=left&amp;title-font-family=sans-serif&amp;title-font-size=13&amp;style=shadowed&amp;bg-color=%2319006d&amp;fg-color=%23ffffff&amp;item-modal=false&amp;item-name=false&amp;size=md&amp;items-alignment=left" style="width:100%;height:600px;display:block;border:none;"></iframe>

<p><em>Landscape of frameworks, tools and hosting spaces for DL</em><sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">5</a></sup></p>

<p>Yet, achieving true framework-agnostic capabilities introduces significant technical complexity despite the trends of the practical engineering solution already being adopted. Abstracting backend operations—such as tensor manipulations, device allocation, and automatic differentiation— without performance penalties or semantic inconsistencies is challenging due to fundamental differences in each framework’s underlying architecture. Deep incompatibilities persist in dynamic graph frameworks (e.g., PyTorch) versus static graph frameworks (e.g., early TensorFlow) or, more impactful nowadays, the functional purity assumptions in JAX in contrast with stateful behaviours in PyTorch and TensorFlow.</p>

<p>As abstraction is not yet a well-adopted solution, multi-backend support is the alternative solution. However, it still carries previous challenges and imposes more effort on researchers and developers as there is a need to implement n times the same functionality (being n the number of backends supported). With pseudo-code duplication and incompatibilities, there is a high risk of impactful bugs being present. Hence, rigorous, automated testing is critical to ensure correctness and reliability in single and multi-framework environments<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>.</p>

<p>Despite these complexities, the ecosystem of tools for model conversion and verification (such as differential testers that compare outputs from two frameworks for the same model) is also developing, pushed by the momentum towards framework-agnostic across the machine learning community. Standardizing functionalities, rigorous testing practices, and robust abstraction layers are increasingly seen as key enablers for sustainable innovation and scalable operationalization of AI technologies in industry and academia.
While the theoretical ‘write-once, run-anywhere’ paradigm for machine learning remains an aspirational goal, ongoing collaboration among framework developers, hardware vendors, and users suggests meaningful progress is already underway.</p>

<hr />

<!-- [^2]: [ACM Transactions on SE and Methodology 2021](https://doi.org/10.1145/3453478) implies that this accelerating rate contributes directly to hidden technical debt, making consistent maintenance, integration and deployment challenging. -->

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>According to <a href="(https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)," title="Mckinsey: Global Survey on the State of AI, 2025">Mckinsey global survey</a> on the corporation view of AI, deep learning and artificial intelligence are integrated into at least one business function by 78% of respondents. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>The paper <a href="https://arxiv.org/pdf/2409.11826">A Taxonomy of Self-Admitted Technical Debt in Deep Learning Systems</a> found that i) there is a significant number of technical debt in all the studied deep learning frameworks. ii) there is design debt, defect debt, documentation debt, test debt, requirement debt, compatibility debt, and algorithm debt in deep learning frameworks. iii) the majority of the technical debt in deep learning framework is design debt (24.07% - 65.27%), followed by requirement debt (7.09% - 31.48%) and algorithm debt (5.62% - 20.67%). In some projects, compatibility debt accounts for more than 10%. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>As noted by McKinsey’s<a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/tech-forward/open-source-in-the-age-of-ai">Open Source in the Age of AI (2023)</a>, around 92% of companies reported that they use open-source software in at least one of their AI initiatives. The survey also highlights that open-source components are increasingly critical not just for research prototyping, but also for production deployments in enterprise settings. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Examples include: i) The ecosystem built over PyTorch: PyTorch Geometric, TorchText, TorchAudio, and third-party libraries like Pyro and Stable Baselines 3 (SB3). ii) The TensorFlow ecosystem: APIs like TensorFlow Lite, TFX, TensorFlow.js, and extensions such as TensorFlow Probability, TensorFlow GNN, and TensorFlow Quantum (plus projects like Sonnet). iii) Other frameworks: Stable Baselines (SB3) is building SBX for reinforcement learning with JAX or <a href="https://deepmind.google/discover/blog/using-jax-to-accelerate-our-research/">Google DeepMind’s libraries</a> including dm-haiku (neural networks), MCTX (Monte Carlo tree search), Jraph (graph neural networks), and physics simulators like JAX MD. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p><a href="https://lfai.landscape2.io/?group=companies-hosting-projects">Open-Source Ecosystem for Machine Learning by the Linux Foundation</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>One recent paper on DL testing <a href="https://dl.acm.org/doi/10.1145/3716497">Deep Learning Library Testing: Definition, Methods, and Challenges</a> Survey with 93 papers collected from the literature, where 69 are related to DL framework testing, 12 to DL compiler testing and 13 to DL hardware library testing. There exists a recent trend with more papers on this topic. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>lucia-ferrer</name></author><category term="deep-learning libraries" /><category term="framework-agnostic support" /><summary type="html"><![CDATA[The exponential growth of deep learning has positioned AI-driven software systems at the forefront of corporate strategy. As these technologies become increasingly integral to business operations, their complexity expands, leading to an unprecedented fragmentation in tools and frameworks — and thus to the necessity of framework-agnostic development.]]></summary></entry></feed>